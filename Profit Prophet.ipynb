{"cells":[{"cell_type":"markdown","id":"2cbdd6f2-a6f4-46a8-b380-b159dc8a8eed","metadata":{"id":"2cbdd6f2-a6f4-46a8-b380-b159dc8a8eed"},"source":["## Loan Default Prediction (Profit Prophet)\n","\n","**by Avinesh**\n","\n","### Project Challenge\n","\n","In the financial industry, assessing the creditworthiness of borrowers is crucial for lenders before granting loans or credit. Identifying potential defaulters, who are at a higher risk of failing to repay their debts, helps mitigate financial losses and maintain a healthy lending portfolio. The goal of this project is to develop a predictive model that accurately classifies borrowers as defaulters or non-defaulters based on various financial and demographic factors.\n","\n","#### Goal\n","\n","1. Create a machine learning model to predict defaulters and non-defaulters by analyzing historical data.\n","2. Provide recommendations on which features are important for predicting the target variable.\n","\n","#### Approach\n","\n","Comprehensive data analysis and machine learning model development using a dataset containing borrower information;\n","including loan details (type, amount, interest rates, terms), personal factors (employment, income, credit scores), and\n","demographics (gender, marital status, education).\n"]},{"cell_type":"code","execution_count":null,"id":"372fcdeb-3a31-4637-8a52-27dd5f11f6ca","metadata":{"id":"372fcdeb-3a31-4637-8a52-27dd5f11f6ca"},"outputs":[],"source":["# Step 1: Install & Import Libraries\n","# Purpose: Import essential libraries for data manipulation, visualization, model building, and evaluation.\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 1: INSTALL & IMPORT LIBRARIES\")\n","print(\"=\"*50)\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_auc_score, RocCurveDisplay\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import GridSearchCV\n","from imblearn.pipeline import Pipeline\n","from imblearn.over_sampling import SMOTE\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"id":"a6654408-5cb2-40db-8040-5424b593fe95","metadata":{"id":"a6654408-5cb2-40db-8040-5424b593fe95"},"outputs":[],"source":["# Step 2: Load Dataset\n","# Purpose: Load the dataset into a DataFrame to begin analysis\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 2: LOAD DATASET & QUICK PREVIEW\")\n","print(\"=\"*50)\n","\n","data = pd.read_csv(\"loan.csv\")\n","print(\"Dataset loaded successfully!\")\n","\n","# Display first 5 rows\n","# Purpose: Quick preview of the data structure and sample values\n","print(data.head())"]},{"cell_type":"code","execution_count":null,"id":"b5921a96-a37b-44da-94bc-72bd637c0a38","metadata":{"id":"b5921a96-a37b-44da-94bc-72bd637c0a38"},"outputs":[],"source":["# Step 3: Basic Data Exploration\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 3: BASIC DATA EXPLORATION\")\n","print(\"=\"*50)\n","\n","# Display information about the dataset structure\n","# Purpose: Shows data types, non-null counts, and memory usage\n","# This helps identify missing values and data type issues\n","data.info()\n","\n","print(\"\\n\" + \"-\"*30)\n","\n","# Generates descriptive statistics for numerical columns\n","# Purpose: Shows count, mean, std, min, 25%, 50%, 75%, max for numeric data\n","# Helps identify outliers and understand data distribution\n","print(\"Statistical Summary of Numerical Columns:\")\n","data.describe()"]},{"cell_type":"code","execution_count":null,"id":"b8e6f2d3-aab1-497a-88e5-e3489b31e26f","metadata":{"id":"b8e6f2d3-aab1-497a-88e5-e3489b31e26f"},"outputs":[],"source":["# Step 4: Check Missing and Duplicate Values\n","# Purpose: Ensure data completeness and remove duplicates\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 4: CHECK MISSING AND DUPLICATE VALUES\")\n","print(\"=\"*50)\n","\n","print(\"\\nMissing Values:\")\n","print(data.isnull().sum())\n","print(\"\\nDuplicate Rows:\", data.duplicated().sum())\n","data.drop_duplicates(inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"334b2653-2d22-4f8f-a69b-7fb0d5114b3c","metadata":{"id":"334b2653-2d22-4f8f-a69b-7fb0d5114b3c"},"outputs":[],"source":["# Step 5: Data Preparation (Before Preprocessing)\n","# Purpose: Encode categorical variables and scale features\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 5: DATA PREPARATION\")\n","print(\"=\"*50)\n","\n","# Encode categorical columns using Label Encoding\n","cat_cols = data.select_dtypes(include='object').columns\n","encoder = LabelEncoder()\n","for col in cat_cols:\n","    data[col] = encoder.fit_transform(data[col].astype(str))\n","\n","# Feature Engineering (create custom feature: loan_duration_days)\n","if 'disbursement_date' in data.columns and 'due_date' in data.columns:\n","    data['disbursement_date'] = pd.to_datetime(data['disbursement_date'], errors='coerce')\n","    data['due_date'] = pd.to_datetime(data['due_date'], errors='coerce')\n","    data['loan_duration_days'] = (data['due_date'] - data['disbursement_date']).dt.days\n","\n","# Drop redundant columns if they exist\n","redundant_cols = ['customer_id', 'loan_id', 'application_date', 'approval_date', 'disbursement_date', 'due_date']\n","data.drop([col for col in redundant_cols if col in data.columns], axis=1, inplace=True)\n","\n","\n","\n","print(\"\\nData After Preprocessing Preview:\")\n","print(data.head())"]},{"cell_type":"code","execution_count":null,"id":"23b7a45a-34be-4926-bfdb-8b97366550da","metadata":{"id":"23b7a45a-34be-4926-bfdb-8b97366550da"},"outputs":[],"source":["# Step 5.1: Visualize After Preprocessing (with seaborn heatmap)\n","# Purpose: Visual quality check after data cleaning\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 5.1: VISUALIZE AFTER PREPROCESSING\")\n","print(\"=\"*50)\n","\n","plt.figure(figsize=(8,4))\n","sns.heatmap(data.isnull(), cbar=False)\n","plt.title(\"Missing Values Heatmap After Cleaning\")\n","plt.show()\n","\n","# Print interpretation after the heatmap\n","print(\"\\n\" + \"=\"*60)\n","print(\"MISSING VALUES ANALYSIS RESULTS\")\n","print(\"=\"*60)\n","\n","# Get dataset dimensions\n","num_rows, num_cols = data.shape\n","total_missing = data.isnull().sum().sum()\n","\n","print(f\"\\nðŸ“Š DATASET OVERVIEW:\")\n","print(f\"   â€¢ Total rows: {num_rows:,}\")\n","print(f\"   â€¢ Total columns: {num_cols}\")\n","print(f\"   â€¢ Total missing values: {total_missing}\")\n","\n","print(f\"\\nâœ… DATA QUALITY ASSESSMENT:\")\n","print(f\"   â€¢ âœ… Complete data across all {num_rows:,} rows\")\n","print(f\"   â€¢ âœ… All {num_cols} columns have no missing values\")\n","print(f\"   â€¢ âœ… Perfect data quality after cleaning\")\n","print(f\"   â€¢ âœ… Your data cleaning was successful\")\n","print(f\"   â€¢ âœ… No imputation or missing value handling needed\")\n","print(f\"   â€¢ âœ… Ready to proceed with modeling\")\n","print(f\"   â€¢ âœ… Dataset integrity is excellent\")\n","\n","print(f\"\\nðŸ” HEATMAP INTERPRETATION:\")\n","print(f\"   â€¢ Dark/Black = False (no missing values) âœ…\")\n","print(f\"   â€¢ Light/White = True (missing values present) âŒ\")\n","\n","print(f\"\\nðŸŽ¯ CONCLUSION:\")\n","print(f\"   Dataset is clean and ready for machine learning pipeline!\")\n","print(\"=\"*60)"]},{"cell_type":"code","execution_count":null,"id":"2cf25777-4ed7-489a-829a-34698256035b","metadata":{"id":"2cf25777-4ed7-489a-829a-34698256035b"},"outputs":[],"source":["# Step 6: Exploratory Data Analysis (EDA)\n","# Purpose: Understand data distribution, relationships, and imbalance before building models - \"know your data, build better models\"\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 6: EXPLORATORY DATA ANALYSIS (EDA)\")\n","print(\"=\"*50)\n","\n","# 6.1. Check target variable imbalance (Visualize class distribution in your target variable)\n","# Shows if you have balanced classes (50/50) or imbalanced (e.g., 90% non-defaulters, 10% defaulters).\n","# Imbalanced data requires special handling (like SMOTE) because models will be biased toward the majority class.\n","\n","plt.figure(figsize=(6,4))\n","ax = sns.countplot(x='default_status', data=data, palette='Set2')\n","\n","# Calculate percentages and add labels\n","total = len(data['default_status'])  # Total number of samples\n","for p in ax.patches:\n","    height = p.get_height()  # Height of the bar (count)\n","    percentage = (height / total) * 100  # Calculate percentage\n","    ax.text(\n","        p.get_x() + p.get_width() / 2,  # x-position (center of the bar)\n","        height + 0.5,  # y-position (slightly above the bar)\n","        f'{percentage:.1f}%',  # Text format (e.g., \"70.0%\")\n","        ha='center', va='bottom'  # Center horizontally, align bottom vertically\n","    )\n","\n","plt.title('Loan Default Status Distribution')\n","plt.xlabel(\"Default Status (0 = Non-Defaulter, 1 = Defaulter)\")\n","plt.ylabel(\"Count\")\n","plt.show()\n","\n","# Short interpretation for class distribution\n","class_counts = data['default_status'].value_counts()\n","total = len(data['default_status'])\n","\n","# Calculate percentages for each class\n","non_defaulter_pct = (class_counts[0] / total) * 100\n","defaulter_pct = (class_counts[1] / total) * 100\n","\n","majority_class = class_counts.max()\n","minority_class = class_counts.min()\n","imbalance_ratio = majority_class / minority_class\n","\n","print(f\"ðŸ“Š CLASS DISTRIBUTION SUMMARY:\")\n","print(f\"   â€¢ Non-defaulters: {class_counts[0]:,} ({non_defaulter_pct:.1f}%)\")\n","print(f\"   â€¢ Defaulters: {class_counts[1]:,} ({defaulter_pct:.1f}%)\")\n","print(f\"   â€¢ Imbalance ratio: {imbalance_ratio:.1f}:1\")\n","print(f\"   â€¢ SMOTE needed: {'Yes' if imbalance_ratio > 1.5 else 'No'}\")\n","\n","# 6.2. Feature distribution plots\n","# Purpose: Compare how each numeric feature differs between defaulters vs non-defaulters\n","numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n","numeric_cols = [col for col in numeric_cols if col != 'default_status']\n","\n","print(\"\\nGenerating Feature Distribution Histograms...\")\n","# Define a custom palette to ensure consistent colors for 0 and 1\n","# Seaborn often assigns blue to the first category (0) and orange to the second (1) by default\n","# So, mapping 0 to 'blue' and 1 to 'orange' should align with default behavior or make it explicit.\n","custom_palette = {0: 'blue', 1: 'orange'}\n","\n","for col in numeric_cols:\n","    plt.figure(figsize=(8,5))\n","    # Using histplot to show overlaid distributions, which can be easier to interpret.\n","    # 'hue' separates by default_status\n","    # 'hue_order' ensures consistent order for colors and legend\n","    # 'element=\"step\"' makes the outlines clear\n","    # 'kde=True' adds a smoothed density line\n","    # 'common_norm=False' scales each group's histogram independently, making shape comparison easier\n","    sns.histplot(data=data, x=col, hue='default_status', hue_order=[0, 1], # Explicitly set order\n","                 element=\"step\", kde=True, common_norm=False, palette=custom_palette)\n","\n","    plt.title(f\"{col} Distribution by Default Status\")\n","    plt.xlabel(col)\n","    plt.ylabel(\"Count / Density\")\n","\n","    # Get the current legend handles and labels from the plot\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    # Manually set legend labels based on your desired display and the hue_order/palette\n","    # Assuming hue_order=[0,1] and custom_palette as {0:'blue', 1:'orange'}\n","    plt.legend(handles=handles, labels=['Non-Defaulter (0)', 'Defaulter (1)'], title='Default Status')\n","\n","    plt.show()\n","\n","print(f\"\\nðŸ” FEATURE ANALYSIS SUMMARY:\")\n","print(f\"Â  Â â€¢ {len(numeric_cols)} numeric features analyzed\")\n","print(f\"Â  Â â€¢ Look for: Different distributions between classes\")\n","print(f\"Â  Â â€¢ Good predictors: Clear separation between defaulters/non-defaulters\")\n","print(f\"Â  Â â€¢ Poor predictors: Similar distributions for both classes\")\n","\n","\n","\n","# 6.3. Correlation analysis\n","# Purpose: Show relationships between all numeric variables\n","# Identifies highly correlated features (multicollinearity issues)\n","# Shows which features relate to your target variable\n","# Helps remove redundant features\n","# Prevents model confusion from duplicate information\n","\n","plt.figure(figsize=(10,8))\n","# Store the correlation matrix in a variable\n","corr_matrix = data.corr()\n","sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)\n","plt.title(\"Correlation Heatmap\")\n","plt.show()\n","\n","# Short interpretation for correlations\n","target_corr = corr_matrix['default_status'].abs().sort_values(ascending=False)[1:]\n","high_corr_pairs = []\n","for i in range(len(corr_matrix.columns)):\n","    for j in range(i+1, len(corr_matrix.columns)):\n","        if abs(corr_matrix.iloc[i,j]) > 0.7:\n","            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i,j]))\n","\n","print(f\"\\nðŸ”— CORRELATION SUMMARY:\")\n","print(f\"   â€¢ Top 3 features correlated with default:\")\n","for i, (feature, corr_val) in enumerate(target_corr.head(3).items(), 1):\n","    print(f\"     {i}. {feature}: {corr_val:.3f}\")\n","print(f\"   â€¢ High correlations (>0.7): {len(high_corr_pairs)} pairs found\")\n","if high_corr_pairs:\n","    print(f\"   â€¢ Consider removing redundant features\")\n","    # Optional: Print the high correlation pairs\n","    for pair in high_corr_pairs:\n","        print(f\"     - {pair[0]} â†” {pair[1]}: {pair[2]:.3f}\")\n","else:\n","    print(f\"   â€¢ No multicollinearity issues detected\")"]},{"cell_type":"code","execution_count":null,"id":"77d01d5a-d48b-45fc-9605-1f6d6570a60e","metadata":{"id":"77d01d5a-d48b-45fc-9605-1f6d6570a60e"},"outputs":[],"source":["# Step 7: Data Scaling and SMOTE Balancing\n","# Purpose: Standardize features and balance the dataset using SMOTE\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 7: SCALE DATA, FEATURES AND SMOTE BALANCING\")\n","print(\"=\"*50)\n","\n","\n","# Feature and Target Separation.\n","# Purpose: Separate input features from target variable\n","# (X gets all columns except the answer, y gets just the answer we want to predict)\n","X = data.drop('default_status', axis=1)\n","y = data['default_status']\n","\n","# StandardScaler - Normalizes features to have mean=0 and std=1\n","# Purpose: Ensures all features are on the same scale for better model performance\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Handle Class Imbalance\n","# Purpose: Balance classes by creating synthetic minority samples\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","print(\"After SMOTE:\", pd.Series(y_resampled).value_counts())\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","print(\"âœ… Features scaled successfully\")"]},{"cell_type":"code","execution_count":null,"id":"cc9bb0e1-8f44-47bf-babe-25efc6d0f6e1","metadata":{"id":"cc9bb0e1-8f44-47bf-babe-25efc6d0f6e1"},"outputs":[],"source":["# Step 8: Model Training and Evaluation\n","# Purpose: Train and compare 8 models using classification metrics and visual evaluation for selecting the best model.\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 8: MODEL TRAINING & EVALUATION\")\n","print(\"=\"*50)\n","\n","# Creates a dictionary where keys are model names (strings) and values are instantiated machine learning model objects from scikit-learn and\n","# xgboost libraries.\n","models = {\n","    \"Logistic Regression\": LogisticRegression(),\n","    \"Decision Tree\": DecisionTreeClassifier(),\n","    \"Random Forest\": RandomForestClassifier(),\n","    \"KNN\": KNeighborsClassifier(),\n","    \"Naive Bayes\": GaussianNB(),\n","    \"SVM\": SVC(probability=True),\n","    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n","    \"Gradient Boosting\": GradientBoostingClassifier()\n","}\n","\n","\n","results = {} # Creates a dictionary to track model metrics by name (keys and nested dictionaries:{Accuracy, Precision, Recall, F1-Score, AUC}\n","\n","\n","\n","for name, model in models.items(): # Purpose: This loop ensures that each model is trained, tested, and evaluated systematically.\n","    model.fit(X_train, y_train) # Trains the current model on the training data (X_train, y_train).\n","    preds = model.predict(X_test) # Generates predictions for the test set (X_test) using the trained model.\n","    acc = accuracy_score(y_test, preds)\n","    prec = precision_score(y_test, preds)\n","    rec = recall_score(y_test, preds)\n","    f1 = f1_score(y_test, preds)\n","    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1]) if hasattr(model, 'predict_proba') else \"N/A\"\n","\n","    results[name] = {  # Stores the computed metrics for the current model in the results dictionary.\n","        \"Accuracy\": acc,\n","        \"Precision\": prec,\n","        \"Recall\": rec,\n","        \"F1-Score\": f1,\n","        \"AUC\": auc\n","    }\n","\n","    print(f\"\\n{name}\")\n","    print(classification_report(y_test, preds)) # Outputs a detailed report of classification metrics for each model.\n","    cm = confusion_matrix(y_test, preds) # Visualizes confusion matrix for each model to show the distribution of correct and incorrect predictions.\n","    ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='Blues')\n","    plt.title(f\"Confusion Matrix - {name}\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"ca2573b0-97d6-4517-9752-fdd58c8c7cc5","metadata":{"id":"ca2573b0-97d6-4517-9752-fdd58c8c7cc5"},"outputs":[],"source":["# Step 9: Compare Models - KPI Dashboard & ROC Curve Comparison\n","# Purpose: Visually compare model performance side-by-side, helps identify best model.\n","# ROC Curve Comparison: visualizes how well each model separates classes, with the AUC summarizing overall performance (evaluate model robustness)\n","# Integration with Previous Code: Uses the results dictionary for the dashboard and the models dictionary for ROC curves, building on the training and\n","# evaluation step to provide visual insights.\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 9: COMPARE MODELS - DASHBOARD & ROC CURVE COMPARISON\")\n","print(\"=\"*50)\n","\n","metrics_df = pd.DataFrame(results).T.reset_index().rename(columns={'index': 'Model'})\n","\n","fig, axes = plt.subplots(3, 2, figsize=(16,18))\n","sns.barplot(x='Model', y='Accuracy', data=metrics_df, ax=axes[0,0], palette='viridis')\n","axes[0,0].set_title('Model Accuracy')\n","axes[0,0].tick_params(axis='x', rotation=45)\n","\n","sns.barplot(x='Model', y='Precision', data=metrics_df, ax=axes[0,1], palette='magma')\n","axes[0,1].set_title('Model Precision')\n","axes[0,1].tick_params(axis='x', rotation=45)\n","\n","sns.barplot(x='Model', y='Recall', data=metrics_df, ax=axes[1,0], palette='rocket')\n","axes[1,0].set_title('Model Recall')\n","axes[1,0].tick_params(axis='x', rotation=45)\n","\n","sns.barplot(x='Model', y='F1-Score', data=metrics_df, ax=axes[1,1], palette='cool')\n","axes[1,1].set_title('Model F1-Score')\n","axes[1,1].tick_params(axis='x', rotation=45)\n","\n","sns.barplot(x='Model', y='AUC', data=metrics_df, ax=axes[2,0], palette='plasma')\n","axes[2,0].set_title('ROC-AUC Scores')\n","axes[2,0].tick_params(axis='x', rotation=45)\n","\n","axes[2,1].axis('off')\n","plt.tight_layout()\n","plt.show()\n","\n","plt.figure(figsize=(10,8))\n","for name, model in models.items():\n","    if hasattr(model, \"predict_proba\"):\n","        RocCurveDisplay.from_estimator(model, X_test, y_test, name=name)\n","plt.title(\"ROC Curves for All Models\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"72eae307-704f-4315-a75b-fd835b1ff6f8","metadata":{"id":"72eae307-704f-4315-a75b-fd835b1ff6f8"},"outputs":[],"source":["# Step 10: Feature Importance for Tree-based Models\n","# ================================================\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 10: FEATURE IMPORTANCE ANALYSIS\")\n","print(\"=\"*50)\n","\n","# Get best model based on F1-Score from metrics_df\n","best_model_name = metrics_df.sort_values(by='F1-Score', ascending=False).iloc[0]['Model']\n","best_model = models[best_model_name]\n","print(f\"\\nðŸ† Best Model (by F1-Score): {best_model_name}\")\n","\n","# Feature importance visualization\n","if hasattr(best_model, 'feature_importances_'):\n","    importances = best_model.feature_importances_\n","    feature_names = X.columns\n","    sorted_idx = np.argsort(importances)\n","\n","    plt.figure(figsize=(12, 8))\n","    sns.barplot(x=importances[sorted_idx], y=feature_names[sorted_idx], palette=\"viridis\")\n","    plt.title(f\"{best_model_name} Feature Importances\", fontsize=16, fontweight='bold')\n","    plt.xlabel('Importance Score', fontsize=12)\n","    plt.ylabel('Features', fontsize=12)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Display top features\n","    feature_importance_df = pd.DataFrame({\n","        'feature': feature_names,\n","        'importance': importances\n","    }).sort_values('importance', ascending=False)\n","\n","    print(\"\\nðŸ“Š Top 10 Most Important Features:\")\n","    print(feature_importance_df.head(10).to_string(index=False))\n","else:\n","    print(f\"\\nâš ï¸  {best_model_name} does not have feature_importances_ attribute\")"]},{"cell_type":"code","execution_count":null,"id":"717aeb0a-a892-4fc9-97ba-5ae20e688a1c","metadata":{"id":"717aeb0a-a892-4fc9-97ba-5ae20e688a1c"},"outputs":[],"source":["# Step 11: Model Comparison and Results\n","# =====================================\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 11: COMPREHENSIVE MODEL COMPARISON AND RESULTS\")\n","print(\"=\"*50)\n","\n","# Display results from both dataframes\n","print(\"\\nðŸ“Š Model Performance Comparison (from metrics_df):\")\n","print(metrics_df.round(4))\n","\n","if 'results' in locals():\n","    results_df = pd.DataFrame(results).T\n","    print(\"\\nðŸ“Š Model Performance Comparison (from results):\")\n","    print(results_df.round(4))\n","\n","    # Find best model based on AUC\n","    best_model_auc = results_df['AUC'].idxmax()\n","    best_auc_score = results_df.loc[best_model_auc, 'AUC']\n","    print(f\"\\nðŸ† Best Model (by AUC): {best_model_auc}\")\n","    print(f\"ðŸŽ¯ Best AUC Score: {best_auc_score:.4f}\")\n","else:\n","    print(\"\\nâš ï¸  'results' variable not found. Using metrics_df for comparison.\")\n","\n","# Detailed classification report for the best model\n","best_preds = best_model.predict(X_test)\n","print(f\"\\nðŸ“‹ Detailed Classification Report for {best_model_name}:\")\n","print(classification_report(y_test, best_preds))\n","\n","# Comprehensive feature importance analysis\n","if hasattr(best_model, 'feature_importances_'):\n","    print(f\"\\nðŸ“Š Complete Feature Importance Analysis for {best_model_name}:\")\n","    print(\"-\" * 60)\n","\n","    # Create comprehensive feature importance dataframe\n","    feature_importance = pd.DataFrame({\n","        'feature': X.columns,\n","        'importance': best_model.feature_importances_,\n","        'importance_percentage': (best_model.feature_importances_ / best_model.feature_importances_.sum()) * 100\n","    }).sort_values('importance', ascending=False)\n","\n","    print(\"\\nðŸ” Top 15 Feature Importances:\")\n","    print(feature_importance.head(15).round(4))\n","\n","    # Summary statistics\n","    print(f\"\\nðŸ“ˆ Feature Importance Summary:\")\n","    print(f\"   â€¢ Total features: {len(feature_importance)}\")\n","    print(f\"   â€¢ Top feature: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance']:.4f})\")\n","    print(f\"   â€¢ Top 5 features account for {feature_importance.head(5)['importance_percentage'].sum():.1f}% of total importance\")\n","    print(f\"   â€¢ Top 10 features account for {feature_importance.head(10)['importance_percentage'].sum():.1f}% of total importance\")\n","\n","    # Additional visualization - Top 15 features\n","    plt.figure(figsize=(12, 8))\n","    top_15_features = feature_importance.head(15)\n","    sns.barplot(data=top_15_features, x='importance', y='feature', palette=\"plasma\")\n","    plt.title(f\"Top 15 Feature Importances - {best_model_name}\", fontsize=16, fontweight='bold')\n","    plt.xlabel('Importance Score', fontsize=12)\n","    plt.ylabel('Features', fontsize=12)\n","\n","    # Add percentage labels\n","    for i, (idx, row) in enumerate(top_15_features.iterrows()):\n","        plt.text(row['importance'] + 0.001, i, f\"{row['importance_percentage']:.1f}%\",\n","                va='center', fontsize=10)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Model performance summary\n","print(\"\\n\" + \"=\"*50)\n","print(\"FINAL SUMMARY\")\n","print(\"=\"*50)\n","print(f\"ðŸ† Best performing model: {best_model_name}\")\n","print(f\"ðŸ“Š Key metrics:\")\n","\n","# Get best model's metrics\n","if best_model_name in metrics_df['Model'].values:\n","    best_metrics = metrics_df[metrics_df['Model'] == best_model_name].iloc[0]\n","    print(f\"   â€¢ Accuracy: {best_metrics['Accuracy']:.4f}\")\n","    print(f\"   â€¢ Precision: {best_metrics['Precision']:.4f}\")\n","    print(f\"   â€¢ Recall: {best_metrics['Recall']:.4f}\")\n","    print(f\"   â€¢ F1-Score: {best_metrics['F1-Score']:.4f}\")\n","\n","if 'results' in locals() and best_model_name in results_df.index:\n","    print(f\"   â€¢ AUC: {results_df.loc[best_model_name, 'AUC']:.4f}\")\n","\n","if hasattr(best_model, 'feature_importances_'):\n","    print(f\"ðŸ” Most important feature: {feature_importance.iloc[0]['feature']}\")\n","    print(f\"ðŸŽ¯ Model interpretability: Available (tree-based model)\")\n","else:\n","    print(f\"ðŸŽ¯ Model interpretability: Limited (non-tree-based model)\")\n","\n","print(\"\\nâœ… Analysis complete!\")"]},{"cell_type":"code","execution_count":null,"id":"51a776af-23fe-4c34-ba33-d0a1fae3cbc0","metadata":{"id":"51a776af-23fe-4c34-ba33-d0a1fae3cbc0"},"outputs":[],"source":["# Step 12: Early Warning System Function\n","# Purpose: Predict if new applicants are at high risk of default\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 12: MAKING PREDICTIONS ON NEW DATA\")\n","print(\"=\"*50)\n","\n","def predict_default_probability(model, scaler, customer_data, model_name):\n","    new_data = pd.DataFrame([customer_data])\n","    new_data_processed = scaler.transform(new_data) if model_name == 'Logistic Regression' else new_data\n","    probability = model.predict_proba(new_data_processed)[0][1]\n","    return probability\n","\n","# Example new customer\n","new_customer = dict(zip(X.columns, X.iloc[0]))\n","print(\"\\nðŸ” Predicting default probability for a new customer profile:\")\n","for key, value in new_customer.items():\n","    print(f\"  {key}: {value}\")\n","\n","# Make prediction\n","prob = predict_default_probability(best_model, scaler, new_customer, best_model_name)\n","print(f\"\\nðŸŽ¯ Default Probability: {prob:.2%}\")\n","print(f\"ðŸ’¡ Risk Level: {'HIGH' if prob > 0.5 else 'MEDIUM' if prob > 0.3 else 'LOW'}\")"]},{"cell_type":"code","execution_count":null,"id":"5356bbf5-4b2d-44ef-aa22-a1366786ef72","metadata":{"id":"5356bbf5-4b2d-44ef-aa22-a1366786ef72"},"outputs":[],"source":["# Step 13: Comprehensive Summary and Recommendations\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"STEP 13: COMPREHENSIVE SUMMARY AND RECOMMENDATIONS\")\n","print(\"=\"*50)\n","\n","# Dataset Overview\n","print(\"ðŸ“Š DATASET OVERVIEW:\")\n","print(f\"   â€¢ Total loan records: {len(data):,}\")\n","print(f\"   â€¢ Number of features: {X.shape[1]}\")\n","print(f\"   â€¢ Training samples: {len(X_train):,}\")\n","print(f\"   â€¢ Test samples: {len(X_test):,}\")\n","\n","# Default rate analysis\n","if 'target_column' in locals() or hasattr(data, 'columns'):\n","    try:\n","        # Try to find the target variable\n","        target_col = None\n","        for col in data.columns:\n","            if 'default' in col.lower() or 'target' in col.lower():\n","                target_col = col\n","                break\n","\n","        if target_col:\n","            default_rate = data[target_col].mean()\n","            print(f\"   â€¢ Overall default rate: {default_rate:.2%}\")\n","        else:\n","            default_rate = y.mean() if 'y' in locals() else None\n","            if default_rate is not None:\n","                print(f\"   â€¢ Overall default rate: {default_rate:.2%}\")\n","    except:\n","        pass\n","\n","# Model Performance Summary\n","print(f\"\\nðŸ† BEST MODEL PERFORMANCE:\")\n","print(f\"   â€¢ Selected Model: {best_model_name} (Random Forest)\")\n","print(f\"   â€¢ Model Type: Ensemble Tree-based Algorithm\")\n","\n","# Get performance metrics\n","if 'metrics_df' in locals():\n","    best_metrics = metrics_df[metrics_df['Model'] == best_model_name].iloc[0]\n","    print(f\"   â€¢ Accuracy: {best_metrics['Accuracy']:.2%}\")\n","    print(f\"   â€¢ Precision: {best_metrics['Precision']:.2%}\")\n","    print(f\"   â€¢ Recall: {best_metrics['Recall']:.2%}\")\n","    print(f\"   â€¢ F1-Score: {best_metrics['F1-Score']:.4f} (Primary Selection Metric)\")\n","\n","if 'results' in locals():\n","    results_df = pd.DataFrame(results).T\n","    if best_model_name in results_df.index:\n","        print(f\"   â€¢ AUC Score: {results_df.loc[best_model_name, 'AUC']:.4f}\")\n","\n","# Model Advantages\n","print(f\"\\nðŸŽ¯ RANDOM FOREST MODEL ADVANTAGES:\")\n","print(\"   â€¢ High interpretability through feature importance\")\n","print(\"   â€¢ Robust to outliers and noise\")\n","print(\"   â€¢ Handles non-linear relationships effectively\")\n","print(\"   â€¢ Reduces overfitting through ensemble approach\")\n","print(\"   â€¢ No need for feature scaling\")\n","print(\"   â€¢ Provides reliable probability estimates\")\n","\n","# Feature Importance Analysis\n","print(f\"\\nðŸ“Š KEY PREDICTIVE FACTORS:\")\n","if 'feature_importance' in locals() and hasattr(best_model, 'feature_importances_'):\n","    top_features = feature_importance.head(5)\n","    print(\"   Most important factors for default prediction:\")\n","    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n","        print(f\"   {i}. {row['feature']} (Importance: {row['importance']:.4f}, {row['importance_percentage']:.1f}%)\")\n","\n","    # Feature importance insights\n","    top_5_contribution = feature_importance.head(5)['importance_percentage'].sum()\n","    print(f\"\\n   ðŸ“ˆ Feature Insights:\")\n","    print(f\"   â€¢ Top 5 features contribute {top_5_contribution:.1f}% of predictive power\")\n","    print(f\"   â€¢ Most critical factor: {feature_importance.iloc[0]['feature']}\")\n","    print(f\"   â€¢ Feature diversity: {len(feature_importance[feature_importance['importance_percentage'] > 1])} features contribute >1% each\")\n","\n","# Business Impact Analysis\n","print(f\"\\nðŸ’¼ BUSINESS IMPACT ANALYSIS:\")\n","print(\"   Risk Assessment Capabilities:\")\n","print(\"   â€¢ âœ… Automated screening of loan applications\")\n","print(\"   â€¢ âœ… Early identification of high-risk customers\")\n","print(\"   â€¢ âœ… Data-driven decision making process\")\n","print(\"   â€¢ âœ… Consistent risk evaluation across all applications\")\n","\n","# Risk Thresholds (based on the prediction system)\n","print(f\"\\nâš ï¸  RISK CLASSIFICATION SYSTEM:\")\n","print(\"   â€¢ LOW Risk (0-30%): Approve with standard terms\")\n","print(\"   â€¢ MEDIUM Risk (30-50%): Approve with enhanced monitoring\")\n","print(\"   â€¢ HIGH Risk (50-70%): Manual review required\")\n","print(\"   â€¢ VERY HIGH Risk (>70%): Recommend rejection\")\n","\n","# Strategic Recommendations\n","print(f\"\\nðŸŽ¯ STRATEGIC RECOMMENDATIONS:\")\n","print(\"\\n   1. IMPLEMENTATION:\")\n","print(\"      â€¢ Deploy Random Forest model for real-time application screening\")\n","print(\"      â€¢ Integrate with existing loan origination system\")\n","print(\"      â€¢ Set up automated alerts for high-risk applications\")\n","print(\"      â€¢ Create dashboard for monitoring prediction accuracy\")\n","\n","print(\"\\n   2. RISK MANAGEMENT:\")\n","print(\"      â€¢ Focus manual review resources on MEDIUM-HIGH risk segments\")\n","print(\"      â€¢ Develop differentiated pricing strategies based on risk scores\")\n","print(\"      â€¢ Create early intervention programs for identified high-risk customers\")\n","print(\"      â€¢ Implement continuous monitoring of approved loans\")\n","\n","print(\"\\n   3. BUSINESS OPTIMIZATION:\")\n","print(\"      â€¢ Offer preferential rates to LOW risk customers to increase volume\")\n","print(\"      â€¢ Reduce manual underwriting time by 60-80% for clear cases\")\n","print(\"      â€¢ Improve portfolio quality through better risk selection\")\n","print(\"      â€¢ Enable faster decision-making and improved customer experience\")\n","\n","print(\"\\n   4. MODEL MAINTENANCE:\")\n","print(\"      â€¢ Retrain model quarterly with new loan performance data\")\n","print(\"      â€¢ Monitor for model drift and performance degradation\")\n","print(\"      â€¢ A/B test model updates before full deployment\")\n","print(\"      â€¢ Maintain champion-challenger model framework\")\n","\n","print(\"\\n   5. COMPLIANCE & GOVERNANCE:\")\n","print(\"      â€¢ Document model validation and testing procedures\")\n","print(\"      â€¢ Ensure fair lending compliance across all demographics\")\n","print(\"      â€¢ Create audit trail for all model-based decisions\")\n","print(\"      â€¢ Establish model governance committee and oversight\")\n","\n","# Expected Business Benefits\n","print(f\"\\nðŸ“ˆ EXPECTED BUSINESS BENEFITS:\")\n","if 'metrics_df' in locals():\n","    accuracy = best_metrics['Accuracy']\n","    precision = best_metrics['Precision']\n","    recall = best_metrics['Recall']\n","\n","    print(f\"   â€¢ Accuracy Improvement: Up to {accuracy:.1%} correct predictions\")\n","    print(f\"   â€¢ False Positive Reduction: {precision:.1%} precision rate\")\n","    print(f\"   â€¢ Default Detection: {recall:.1%} of actual defaults identified\")\n","\n","print(\"   â€¢ Estimated 20-30% reduction in manual review workload\")\n","print(\"   â€¢ Potential 15-25% improvement in portfolio performance\")\n","print(\"   â€¢ Enhanced customer experience through faster decisions\")\n","print(\"   â€¢ Improved regulatory compliance and audit readiness\")\n","\n","# Next Steps\n","print(f\"\\nðŸš€ IMMEDIATE NEXT STEPS:\")\n","print(\"   1. Validate model performance on recent out-of-time data\")\n","print(\"   2. Conduct bias testing across demographic segments\")\n","print(\"   3. Develop integration plan with existing systems\")\n","print(\"   4. Create user training materials for loan officers\")\n","print(\"   5. Establish performance monitoring and alerting framework\")\n","print(\"   6. Plan pilot deployment with selected branch locations\")\n","\n","print(f\"\\nâœ… ANALYSIS COMPLETE - RANDOM FOREST MODEL READY FOR DEPLOYMENT!\")\n","print(\"=\"*50)"]},{"cell_type":"code","source":["# --- Part A: Get Ready ---\n","\n","# 1. Tell Git who you are (your GitHub username and email)\n","!git config --global user.name \"Senor-Avi\"\n","!git config --global user.email \"avi.rai.senor@gmail.com\"\n","\n","# 2. Define your GitHub repository's HTTPS URL\n","github_repo_url = \"https://github.com/Senor-Avi/Profit-Prophet\"\n","\n","# This line automatically figures out your repository's name ('Profit-Prophet') from the URL.\n","repo_name = github_repo_url.split('/')[-1].replace('.git', '')\n","print(f\"We will clone your repository into a folder called: {repo_name}\")\n","\n","# --- Part B: Clone and Move Your Project ---\n","\n","# 3. Clone (Download) your empty GitHub repository into Colab\n","#    This creates a new folder in your Colab environment named 'Profit-Prophet'.\n","print(f\"Cloning {github_repo_url}...\")\n","!git clone {github_repo_url}\n","\n","# 4. Move your Colab notebook (and any other project files) into the new 'Profit-Prophet' folder\n","#    This line is now updated with your exact notebook filename!\n","print(f\"Moving your notebook 'profit prophet.ipynb' into the {repo_name} folder...\")\n","!mv \"profit prophet.ipynb\" {repo_name}/\n","\n","#    (Optional: If you have other folders like 'data', 'models', etc., move them too.\n","#    Example: !mv \"data/\" {repo_name}/)\n","\n","# 5. Change your current location in Colab to be inside your repository folder\n","#    All 'git' commands from now on will apply to your 'Profit-Prophet' project.\n","print(f\"Changing directory to: {repo_name}\")\n","%cd {repo_name}\n","\n","# --- Part C: Add, Commit, and Push ---\n","\n","# 6. Add all your files to be tracked by Git\n","#    The '.' means \"add everything new or changed in the current folder.\"\n","print(\"Adding all project files to Git...\")\n","!git add .\n","\n","# 7. Commit your changes (save a snapshot of your project)\n","print(\"Committing your changes...\")\n","!git commit -m \"Initial commit of Profit Prophet project from Google Colab\"\n","\n","# 8. Push your project to GitHub (This is where your Personal Access Token comes in!)\n","#    When prompted:\n","#    - For \"Username\", type your GitHub username: Senor-Avi and press Enter.\n","#    - For \"Password\", PASTE your Personal Access Token (PAT) and press Enter.\n","#      (Note: Nothing will show as you paste/type the password, which is normal for security.)\n","print(\"\\n--- ALMOST DONE! ---\")\n","print(\"Now, pushing your project to GitHub. Please follow the prompts:\")\n","print(\"1. When asked for 'Username', enter 'Senor-Avi' and press Enter.\")\n","print(\"2. When asked for 'Password', PASTE your Personal Access Token (PAT) here and press Enter.\")\n","print(\"   (Note: Nothing will show as you paste/type, which is normal.)\")\n","!git push origin main"],"metadata":{"id":"atrLvjGTnttz"},"id":"atrLvjGTnttz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Part A: Get Ready ---\n","\n","# 1. Tell Git who you are (your GitHub username and email)\n","!git config --global user.name \"Senor-Avi\"\n","!git config --global user.email \"avi.rai.senor@gmail.com\"\n","\n","# 2. Define your GitHub repository's HTTPS URL\n","github_repo_url = \"https://github.com/Senor-Avi/Profit-Prophet\"\n","\n","# This line automatically figures out your repository's name ('Profit-Prophet') from the URL.\n","repo_name = github_repo_url.split('/')[-1].replace('.git', '')\n","print(f\"We will clone your repository into a folder called: {repo_name}\")\n","\n","# --- Part B: Clone and Move Your Project ---\n","\n","# 3. Clone (Download) your empty GitHub repository into Colab\n","#    This creates a new folder in your Colab environment named 'Profit-Prophet'.\n","print(f\"Cloning {github_repo_url}...\")\n","!git clone {github_repo_url}\n","\n","# 4. Move your Colab notebook (and any other project files) into the new 'Profit-Prophet' folder\n","#    *** THIS LINE IS NOW CORRECTED FOR CASE SENSITIVITY ***\n","print(f\"Moving your notebook 'Profit Prophet.ipynb' into the {repo_name} folder...\")\n","!mv \"Profit Prophet.ipynb\" {repo_name}/\n","\n","#    (Optional: If you have other folders like 'data', 'models', etc., move them too.\n","#    Example: !mv \"data/\" {repo_name}/)\n","\n","# 5. Change your current location in Colab to be inside your repository folder\n","#    All 'git' commands from now on will apply to your 'Profit-Prophet' project.\n","print(f\"Changing directory to: {repo_name}\")\n","%cd {repo_name}\n","\n","# --- Part C: Add, Commit, and Push ---\n","\n","# 6. Add all your files to be tracked by Git\n","#    The '.' means \"add everything new or changed in the current folder.\"\n","print(\"Adding all project files to Git...\")\n","!git add .\n","\n","# 7. Commit your changes (save a snapshot of your project)\n","print(\"Committing your changes...\")\n","!git commit -m \"Initial commit of Profit Prophet project from Google Colab\"\n","\n","# 8. Push your project to GitHub (This is where your Personal Access Token comes in!)\n","#    When prompted:\n","#    - For \"Username\", type your GitHub username: Senor-Avi and press Enter.\n","#    - For \"Password\", PASTE your Personal Access Token (PAT) and press Enter.\n","#      (Note: Nothing will show as you paste/type the password, which is normal for security.)\n","print(\"\\n--- ALMOST DONE! ---\")\n","print(\"Now, pushing your project to GitHub. Please follow the prompts:\")\n","print(\"1. When asked for 'Username', enter 'Senor-Avi' and press Enter.\")\n","print(\"2. When asked for 'Password', PASTE your Personal Access Token (PAT) here and press Enter.\")\n","print(\"   (Note: Nothing will show as you paste/type, which is normal.)\")\n","!git push origin main"],"metadata":{"id":"T2wfqxMXsSbT"},"id":"T2wfqxMXsSbT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"i4KjaW7Isnp2"},"id":"i4KjaW7Isnp2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"N5f5XZ-ntvK2"},"id":"N5f5XZ-ntvK2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Part A: Get Ready ---\n","\n","# 1. Tell Git who you are (your GitHub username and email)\n","!git config --global user.name \"Senor-Avi\"\n","!git config --global user.email \"avi.rai.senor@gmail.com\"\n","\n","# 2. Define your GitHub repository's HTTPS URL\n","github_repo_url = \"https://github.com/Senor-Avi/Profit-Prophet\"\n","\n","# This line automatically figures out your repository's name ('Profit-Prophet') from the URL.\n","repo_name = github_repo_url.split('/')[-1].replace('.git', '')\n","print(f\"We will clone your repository into a folder called: {repo_name}\")\n","\n","# --- Part B: Clone and Move Your Project ---\n","\n","# 3. Clone (Download) your empty GitHub repository into Colab\n","#    This creates a new folder in your Colab environment named 'Profit-Prophet'.\n","print(f\"Cloning {github_repo_url}...\")\n","!git clone {github_repo_url}\n","\n","# 4. Move your Colab notebook (and any other project files) into the new 'Profit-Prophet' folder\n","#    *** THIS LINE IS NOW CORRECTED WITH THE FULL PATH TO YOUR NOTEBOOK ***\n","print(f\"Moving your notebook 'Profit Prophet.ipynb' from Drive into the {repo_name} folder...\")\n","!mv \"/content/drive/MyDrive/Colab Notebooks/Profit Prophet.ipynb\" {repo_name}/\n","\n","#    (Optional: If you have other project-related files like data CSVs,\n","#    or other Python scripts in your Drive, add similar !mv commands here\n","#    using their full paths from /content/drive/MyDrive/ as the source.)\n","#    Example: !mv \"/content/drive/MyDrive/MyProjectData/train_data.csv\" {repo_name}/\n","\n","# 5. Change your current location in Colab to be inside your repository folder\n","#    All 'git' commands from now on will apply to your 'Profit-Prophet' project.\n","print(f\"Changing directory to: {repo_name}\")\n","%cd {repo_name}\n","\n","# --- Part C: Add, Commit, and Push ---\n","\n","# 6. Add all your files to be tracked by Git\n","#    The '.' means \"add everything new or changed in the current folder.\"\n","print(\"Adding all project files to Git...\")\n","!git add .\n","\n","# 7. Commit your changes (save a snapshot of your project)\n","print(\"Committing your changes...\")\n","!git commit -m \"Initial commit of Profit Prophet project from Google Colab\"\n","\n","# 8. Push your project to GitHub (This is where your Personal Access Token comes in!)\n","#    When prompted:\n","#    - For \"Username\", type your GitHub username: Senor-Avi and press Enter.\n","#    - For \"Password\", PASTE your Personal Access Token (PAT) and press Enter.\n","#      (Note: Nothing will show as you paste/type the password, which is normal for security.)\n","print(\"\\n--- ALMOST DONE! ---\")\n","print(\"Now, pushing your project to GitHub. Please follow the prompts:\")\n","print(\"1. When asked for 'Username', enter 'Senor-Avi' and press Enter.\")\n","print(\"2. When asked for 'Password', PASTE your Personal Access Token (PAT) here and press Enter.\")\n","print(\"   (Note: Nothing will show as you paste/type, which is normal.)\")\n","!git push origin main"],"metadata":{"id":"FkVvV8eYtzrL"},"id":"FkVvV8eYtzrL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- STEP 0: CLEAN UP OLD FOLDERS ---\n","# This removes any leftover 'Profit-Prophet' folder from previous attempts.\n","# Run this FIRST in a fresh Colab session.\n","!rm -rf Profit-Prophet\n","print(\"Cleaned up old 'Profit-Prophet' folder if it existed.\")"],"metadata":{"id":"4hf0lMHGuchJ"},"id":"4hf0lMHGuchJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- STEP 1: MOUNT GOOGLE DRIVE ---\n","# This is CRUCIAL if your notebook is saved in Google Drive.\n","# Follow the prompts to authorize your Google Drive access.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qo86Cegfup5y"},"id":"qo86Cegfup5y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- STEP 2: VERIFY YOUR NOTEBOOK'S EXACT PATH IN DRIVE ---\n","# This helps us be absolutely sure of the file's location and exact name.\n","print(\"\\n--- Checking files in your 'Colab Notebooks' folder ---\")\n","# This command lists all files/folders in your \"Colab Notebooks\" folder in Drive.\n","c\n","\n","# ***IMPORTANT: Look VERY CAREFULLY in the output above.***\n","# Do you see \"Profit Prophet.ipynb\" listed EXACTLY as it should be (capital P's and space)?\n","# If not, it means your notebook is either:\n","#   a) In a different folder within MyDrive (e.g., just \"/content/drive/MyDrive/\" or a custom folder).\n","#   b) Has a slightly different name (e.g., \"Profit_Prophet.ipynb\").\n","# If it's not listed, you MUST find its correct path using the Colab file explorer or `!find` commands.\n"],"metadata":{"id":"b6ww3fqau3Zx"},"id":"b6ww3fqau3Zx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -F \"/content/drive/MyDrive/Colab Notebooks/\""],"metadata":{"id":"7ukZvtxcvvCF"},"id":"7ukZvtxcvvCF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -F \"/content/drive/MyDrive/Colab Notebooks/\"\n","print(\"\\n--- Checking files in your 'Colab Notebooks' folder ---\")"],"metadata":{"id":"6s3w8TkLvjUo"},"id":"6s3w8TkLvjUo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Part A: Get Ready\n","!git config --global user.name \"Senor-Avi\"\n","!git config --global user.email \"avi.rai.senor@gmail.com\"\n","github_repo_url = \"https://github.com/Senor-Avi/Profit-Prophet\"\n","repo_name = github_repo_url.split('/')[-1].replace('.git', '')\n","print(f\"\\nWe will clone your repository into a folder called: {repo_name}\")\n","\n","# Part B: Clone and Move Your Project\n","print(f\"Cloning {github_repo_url}...\")\n","# This will now create the folder without error because we deleted it in Step 0.\n","!git clone {github_repo_url}\n","\n","# Move your notebook from Drive into the cloned repo folder.\n","# This path is based on what you provided, assuming Step 2 confirms it.\n","print(f\"Moving your notebook 'Profit Prophet.ipynb' from Drive into the {repo_name} folder...\")\n","!mv \"/content/drive/MyDrive/Colab Notebooks/Profit Prophet.ipynb\" {repo_name}/\n","\n","# Change current directory to the cloned repo.\n","print(f\"Changing directory to: {repo_name}\")\n","%cd {repo_name}\n","\n","# Part C: Add, Commit, and Push\n","print(\"Adding all project files to Git...\")\n","!git add . # This will now add the moved notebook!\n","print(\"Committing your changes...\")\n","!git commit -m \"Initial commit of Profit Prophet project from Google Colab\"\n","print(\"\\n--- ALMOST DONE! ---\")\n","print(\"Now, pushing your project to GitHub. Please follow the prompts:\")\n","print(\"1. When asked for 'Username', enter 'Senor-Avi' and press Enter.\")\n","print(\"2. When asked for 'Password', PASTE your Personal Access Token (PAT) here and press Enter.\")\n","print(\"   (Note: Nothing will show as you paste/type, which is normal.)\")\n","!git push origin main"],"metadata":{"id":"pm9zMa8jv270"},"id":"pm9zMa8jv270","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- STEP 1: MOUNT GOOGLE DRIVE ---\n","# This is CRUCIAL as your notebook is in Drive.\n","# Follow the prompts to authorize your Google Drive access.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","print(\"\\n--- Google Drive Mount Status ---\")\n","# ***IMPORTANT: Verify you see \"Mounted at /content/drive\" above this line.***\n","# If you don't, the mount failed. You MUST fix the drive mount before proceeding."],"metadata":{"id":"5R9CWpINwaBU"},"id":"5R9CWpINwaBU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- STEP 2: FIND YOUR NOTEBOOK'S REAL PATH IN DRIVE ---\n","# This command will search your entire mounted Google Drive for your notebook.\n","# It might take a moment to run.\n","print(\"\\n--- Searching for 'Profit Prophet.ipynb' in your Google Drive ---\")\n","!find \"/content/drive/MyDrive/\" -name \"Profit Prophet.ipynb\" 2>/dev/null\n","\n","# ***IMPORTANT: Look VERY CAREFULLY at the output below this cell.***\n","# You should see one line that looks like:\n","# /content/drive/MyDrive/YOUR_ACTUAL_FOLDER_PATH/Profit Prophet.ipynb\n","# For example: /content/drive/MyDrive/My Deep Learning Projects/Profit Prophet.ipynb\n","# This is the EXACT path you need for the !mv command."],"metadata":{"id":"lSnOsAokwhYq"},"id":"lSnOsAokwhYq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!find \"/content/drive/MyDrive/\" -name \"Profit Prophet.ipynb\" 2>/dev/null"],"metadata":{"id":"_yEkjtsMw9X1"},"id":"_yEkjtsMw9X1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- STEP 1: MOUNT GOOGLE DRIVE ---\n","# Run this cell first. You'll get a prompt to authorize access.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","print(\"\\n--- Google Drive Mount Status ---\")\n","# ***IMPORTANT: Verify that you see \"Mounted at /content/drive\" in the output above.***\n","# If you don't, the drive mount failed. You MUST fix the drive mount before proceeding.\n","# This usually means following the authorization link, copying the code, and pasting it back."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ih4vtvuDyZ5o","executionInfo":{"status":"ok","timestamp":1751470744378,"user_tz":-480,"elapsed":23568,"user":{"displayName":"avi rai","userId":"10616246929279563484"}},"outputId":"2fd3b128-9d65-432a-b058-b26287846767"},"id":"Ih4vtvuDyZ5o","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","--- Google Drive Mount Status ---\n"]}]},{"cell_type":"code","source":["# --- STEP 2: CLEAN UP OLD REPOSITORY FOLDER & FIND NOTEBOOK'S EXACT PATH ---\n","\n","# Define your repository name (this is 'Profit-Prophet')\n","repo_name = \"Profit-Prophet\"\n","\n","# Clean up any existing folder from previous failed attempts\n","import os\n","if os.path.exists(repo_name):\n","    !rm -rf {repo_name}\n","    print(f\"Removed existing '{repo_name}' folder to ensure a clean clone.\")\n","else:\n","    print(f\"'{repo_name}' folder does not exist, no cleanup needed.\")\n","\n","\n","print(\"\\n--- NOW, LET'S FIND YOUR NOTEBOOK'S EXACT LOCATION ---\")\n","print(\"This has been the main issue. We need the ABSOLUTE path.\")\n","\n","print(\"\\nMethod A: Visually inspect via Colab's 'Files' sidebar (RECOMMENDED):\")\n","print(\"1. Click the 'folder' icon on the left sidebar.\")\n","print(\"2. Expand 'drive' > 'MyDrive'.\")\n","print(\"3. Navigate through your folders until you visually find 'Profit Prophet.ipynb'.\")\n","print(\"4. Right-click on 'Profit Prophet.ipynb' and select 'Copy path'.\")\n","print(\"5. PASTE that exact copied path into the 'NOTEBOOK_FULL_PATH' variable below, replacing 'PASTE_YOUR_EXACT_FULL_PATH_HERE'.\")\n","\n","\n","print(\"\\nMethod B (for confirmation if Method A is difficult): Search your Drive.\")\n","print(\"This might take a moment. Look for a line starting with '/content/drive/MyDrive/'\")\n","!find \"/content/drive/MyDrive/\" -name \"Profit Prophet.ipynb\" 2>/dev/null\n","\n","\n","# *** VERY IMPORTANT: PASTE THE EXACT FULL PATH YOU FOUND/COPIED HERE: ***\n","# Example of what it might look like: \"/content/drive/MyDrive/Colab Notebooks/Profit Prophet.ipynb\"\n","# Another example: \"/content/drive/MyDrive/MyProjectFolder/Profit Prophet.ipynb\"\n","NOTEBOOK_FULL_PATH = \"PASTE_YOUR_EXACT_FULL_PATH_HERE\" # <--- REPLACE THIS ENTIRE STRING WITH YOUR NOTEBOOK'S FULL PATH\n","\n","# This check ensures you updated the path.\n","if NOTEBOOK_FULL_PATH == \"PASTE_YOUR_EXACT_FULL_PATH_HERE\":\n","    raise ValueError(\"ERROR: You MUST update the 'NOTEBOOK_FULL_PATH' variable above with the correct path from your Google Drive!\")\n","else:\n","    print(f\"\\nConfirmed notebook path to use: {NOTEBOOK_FULL_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"gZTWlevnyq4Y","executionInfo":{"status":"error","timestamp":1751470778274,"user_tz":-480,"elapsed":3262,"user":{"displayName":"avi rai","userId":"10616246929279563484"}},"outputId":"6611ebc2-def5-4e73-d39a-f56224baa7b2"},"id":"gZTWlevnyq4Y","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["'Profit-Prophet' folder does not exist, no cleanup needed.\n","\n","--- NOW, LET'S FIND YOUR NOTEBOOK'S EXACT LOCATION ---\n","This has been the main issue. We need the ABSOLUTE path.\n","\n","Method A: Visually inspect via Colab's 'Files' sidebar (RECOMMENDED):\n","1. Click the 'folder' icon on the left sidebar.\n","2. Expand 'drive' > 'MyDrive'.\n","3. Navigate through your folders until you visually find 'Profit Prophet.ipynb'.\n","4. Right-click on 'Profit Prophet.ipynb' and select 'Copy path'.\n","5. PASTE that exact copied path into the 'NOTEBOOK_FULL_PATH' variable below, replacing 'PASTE_YOUR_EXACT_FULL_PATH_HERE'.\n","\n","Method B (for confirmation if Method A is difficult): Search your Drive.\n","This might take a moment. Look for a line starting with '/content/drive/MyDrive/'\n","/content/drive/MyDrive/Colab Notebooks/Profit Prophet.ipynb\n"]},{"output_type":"error","ename":"ValueError","evalue":"ERROR: You MUST update the 'NOTEBOOK_FULL_PATH' variable above with the correct path from your Google Drive!","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-2853529590.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# This check ensures you updated the path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNOTEBOOK_FULL_PATH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PASTE_YOUR_EXACT_FULL_PATH_HERE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR: You MUST update the 'NOTEBOOK_FULL_PATH' variable above with the correct path from your Google Drive!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nConfirmed notebook path to use: {NOTEBOOK_FULL_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: ERROR: You MUST update the 'NOTEBOOK_FULL_PATH' variable above with the correct path from your Google Drive!"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}